{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###                 MULTIVARIATE LINEAR REGRESSION [OOP Edition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@# nope!11!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!This notebook is divided into two parts. \"Model 1\" is my rendition of the MLR assignment.\n",
    "\"Model 2\" is the OOP version of the former.\n",
    "Both endeavour to predict employee salaries from different employee characteristics (or features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "sns.set()\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "# from statsmodels.formula.api import ols\n",
    "import statsmodels\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **This assignment addresses the following:**\n",
    "\n",
    "[OOP for Data Science](#oop)\n",
    "<a href='oop'> </a>\n",
    "\n",
    "[Exploratory Data Analysis -EDA](#EDA) \n",
    "<a href='EDA'> </a>\n",
    "\n",
    "[Feature Engineering](#feng)\n",
    "<a href='feng'> </a>\n",
    "\n",
    "[Correlation and Statistical Significance Analysis](#corr)\n",
    "<a href='corr'> </a>\n",
    "\n",
    "[~~Feature Selection~~](#select)\n",
    "<a href='select'> </a>\n",
    "\n",
    "[Model Training](#train)\n",
    "<a href='train'> </a>\n",
    "\n",
    "[Predictions](#predict)\n",
    "<a href='predict'> </a>\n",
    "\n",
    "[Model Evaluation](#eval)\n",
    "<a href='eval'> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MODEL 1](#MLR)   \n",
    "\n",
    "\n",
    "\n",
    "## [MODEL 2](#OOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Plotter import Histogram_Plotter, Scatter_Plot c\n",
    "\n",
    "### Use the model you built in the MLR project to predict log-transformed salary (log_salary). Also create a second multiple regression model which does not include yearsrank as a feature. Save these model instances as model1 and model2. Remember to scale (standardise) the features before modelling. m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oop = pd.read_csv('data/salary.csv')\n",
    "oop = oop.drop(\"yearsrank\", axis=1)\n",
    "\n",
    "#Method to fix missing value\n",
    "oop['salary'].fillna(oop['salary'].mean(),inplace = True)\n",
    "\n",
    "certain markdowns have been omitted in model 1 because they would be repetitive from model 2\n",
    "\n",
    "oop['log_salary'] = np.log(oop.salary)\n",
    "Logarithmic transformations are used to make  better predicted outcomes from a linear reg model.\n",
    "\n",
    "dummies = pd.get_dummies(oop['Field'])\n",
    "\n",
    "dum = dummies.rename(columns={1:'engineering',2:'finance',3:'HR',4:'marketing'})\n",
    "\n",
    "oop2 = pd.concat([oop, dum] ,axis=1, ignore_index=False)\n",
    "oop2.drop(\"Field\", inplace=True, axis=1)\n",
    "\n",
    "### Train Test Split for Model 2 \n",
    "\n",
    "#split data\n",
    "x2 = oop.drop(['salary', 'log_salary'], axis=1)\n",
    "y2 = oop[\"log_salary\"]\n",
    "\n",
    "x2 = sm.add_constant(x2)\n",
    "\n",
    "X_train2,X_test2,Y_train2,Y_test2 = train_test_split(x2,y2,test_size=0.3, random_state=50)\n",
    "\n",
    "model2 = sm.OLS(Y_train2, X_train2).fit()\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "### MSE\n",
    "errors_m1 = ErrorCalculator(Y_test, model1.predict(sm.add_constant(X_test)))\n",
    "m1_mse = errors_m1.get_mse()\n",
    "errors_m2 = ErrorCalculator(Y_test2, model2.predict(sm.add_constant(X_test2)))\n",
    "m2_mse = errors_m2.get_mse()\n",
    "\n",
    "print(f'Model1 MSE: {m1_mse}')\n",
    "print(f'Model2 MSE: {m2_mse}')\n",
    "print(f'MSE diff: {m2_mse - m1_mse}')\n",
    "\n",
    "### RMSE\n",
    "\n",
    "rmse_m1 = errors_m1.get_rmse()\n",
    "rmse_m2 = errors_m2.get_rmse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model1 RMSE: {rmse_m1}')\n",
    "print(f'Model2 RMSE: {rmse_m2}')\n",
    "print(f'MSE diff: {rmse_m2 - rmse_m1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model2 has a higher RMSE. This can be attributed to a different feature being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Histogram_Plotter import Histogram_Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_hist = Histogram_Plotter(Y_test, model1.predict(sm.add_constant(X_test)))\n",
    "model1_scatter = Scatter_Plot(Y_test, model1.predict(sm.add_constant(X_test)))\n",
    "\n",
    "# Mode1 Histogram Plot\n",
    "model1_hist.plot()\n",
    "\n",
    "#mode2l1 Scatter Plot\n",
    "model1_scatter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_hist = Histogram_Plotter(Y_test2, model2.predict(sm.add_constant(X_test2)))\n",
    "model2_scatter = Scatter_Plot(Y_test2, model2.predict(sm.add_constant(X_test2)))\n",
    "\n",
    "# Mode1 Histogram Plot\n",
    "model2_hist.plot()\n",
    "\n",
    "#mode2l1 Scatter Plot\n",
    "model1_scatter.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter():\n",
    "    def __init__(self, y, y_pred):\n",
    "        self.y = y\n",
    "        self.y_pred = y_pred\n",
    "        \n",
    "    def run_calcs(self):\n",
    "        return self.y - self.y_pred\n",
    "    \n",
    "    def plot(self):\n",
    "        f, axes = plt.subplots(1, 2,figsize=(15, 5))\n",
    "        grid = plt.GridSpec(1, 2, wspace=0.5, hspace=0.3)\n",
    "        plt.subplot(grid[0, 0])\n",
    "        plt.hist(self.y - self.y_pred)\n",
    "        plt.xlabel('residuals')\n",
    "        plt.ylabel('frequency')\n",
    "        return plt.show()\n",
    "\n",
    "class Histogram_Plotter(Plotter):\n",
    "    def __init__(self, y, y_pred):\n",
    "        Plotter.__init__(self, y, y_pred)\n",
    "\n",
    "class Scatter_Plot(Plotter):\n",
    "    def __init__(self, y, y_pred):\n",
    "        Plotter.__init__(self, y, y_pred)\n",
    "\n",
    "    def plot(self):\n",
    "        df = pd.DataFrame({\"Actual\": self.y, \n",
    "                            \"Predicted\": self.y_pred})\n",
    "        df.plot.scatter(x=\"Actual\", y=\"Predicted\")\n",
    "        plt.title(\"Predicted vs Actual Values\")\n",
    "        plt.xlabel(\"Actual\")\n",
    "        plt.ylabel(\"Prediction\")\n",
    "        return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='EDA'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Exploratory Data Analysis (EDA)\n",
    "- Data Ingestion\n",
    "- Data Preprocessing\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "sal = pd.read_csv('data/salary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine DataSet\n",
    "sal.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our target variable (salary) has one missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to fix missing value\n",
    "sal['salary'].fillna(sal['salary'].mean(),inplace = True)\n",
    "#sal = sal.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Missing value has been filled w/ mean. Duplicates would be dropped if dataset had any \n",
    "* Filled w/ mean instead of dropping because salary is the target variable and don't want to miss any insights it might bring even when it's just one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above table tells us the number of observations (514). We expect salary to have 514 because the missing value is now represented by the mean.\n",
    "* The descriptive statistics table also tells us about the means, standard deviations, min and max values as well as the percentiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='feng'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Feature Engineering\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "field = ['Engineering', 'Finance', 'Human Resources', 'Marketing']\n",
    "trace = go.Pie(labels = field, values = sal.Field)\n",
    "data = [trace]\n",
    "layout = go.Layout(\n",
    "   {\"title\":\"Career Fields\"})\n",
    "fig = go.Figure(data,layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this graph is to have an understanding of which career fields are the most dominant within the dataset. The Marketing profession accounts for 33.3%, Eng & HR are tied at second place leaving Finance in last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(sal['Field'])\n",
    "\n",
    "dum = dummies.rename(columns={1:'engineering',2:'finance',3:'HR',4:'marketing'})\n",
    "\n",
    "sal2 = pd.concat([sal, dum] ,axis=1, ignore_index=False)\n",
    "sal2.drop(\"Field\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='corr'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Correlation Analysis and Statistical Significance \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Model 1 Features:__  exprior, yearsworked, yearsrank, market, degree, otherqual, position, male,\n",
    "Field_dummyvariable1, Field_dummyvariable2, Field_dummyvariable3, yearsabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sal_corr = sal2.corr()\n",
    "sal_corr.style.background_gradient(cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Everything has a correlation but it is the strength of the correlation that we are interested in.__\n",
    "* Based on the above table these features are good predictors for salary; yearsworked, yearsrank, position and Field\n",
    "\n",
    "* As per assignment instructions ALL features will be used for Model 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='select'></a> <div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "      N/A FOR THIS ASSIGNMENT!  \n",
    "\n",
    "# ~~Feature Selection~~\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='train'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Model Training\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dataset is split into a train & test set. \n",
    "70% of the data will go into the training set and the remaining 30% will be used for testing.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal2['log_salary'] = np.log(sal2.salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "x = sal2.drop(['salary','log_salary'] , axis=1)\n",
    "y = sal2[\"log_salary\"]\n",
    "\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = sm.OLS(Y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared value reveals the quality of the regression model. It describes the relationships between dependent and independent variables in a model. The R-squared value for this model was 0.66. Now, having used more features & did log transformation, it is now 0.83.\n",
    "This means the accuracy of the model is approx 83%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='predict'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Predictions & Model Testing\n",
    "\n",
    "\n",
    "Predicting salary using the test set.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_salary = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the __X_test__ data to pass in features the model has never seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe for predicted values\n",
    "df = pd.DataFrame(predict_salary)\n",
    "df.rename(columns={0:'predicted_salary'},inplace=True)\n",
    "# df = df.astype(int)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.concat([Y_test,df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Distribution of Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot((Y_test - predict_salary))\n",
    "plt.title(\"Distribution of Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The figure is normally distributed\n",
    "- This is a good sign because it means this model is a correct choice for the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Model Evaluations\n",
    "Regression Evaluation Metrics\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 common evaluation metrics for regression problems:\n",
    "\n",
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "All of these are **loss functions**. We want to minimize them to create the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:', metrics.mean_squared_error(Y_test, predict_salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('testRMSE:', np.sqrt(metrics.mean_squared_error(Y_test, predict_salary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trainsetRMSE:', np.sqrt(metrics.mean_squared_error(Y_train, model1.predict(X_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RMSE indicates the absolute fit of the model to the data (how close the actual data points are to the model's predicted ones.)\n",
    "- Test RMSE is bigger than the Trainset RMSE\n",
    "- RMSE is a good measure of how accurately the model predicts the target variable. It is the most important criteria for fit if the main purpose of the model is prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error(MAE) is one of the many metrics for summarizing and assessing the quality of a machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print('MAE:', metrics.mean_absolute_error(Y_test, predict_salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(Y_test, predict_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mean absolute percentage error (MAPE) is a statistical measure of how accurate a forecast system is. It measures this accuracy as a percentage.\n",
    "- (MAPE) works best if there are no extremes to the data\n",
    "- Since MAPE is a measure of error, high numbers are bad and low numbers are good. Initially mape % was 12.2 but now it has reduced to 1.1% post-log transformation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(Y_test, predict_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Error Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import Lasso\n",
    "# from yellowbrick.regressor import PredictionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod = Lasso()\n",
    "# visualizer = PredictionError(mod)\n",
    "\n",
    "# #Fit training data to visualizer\n",
    "# visualizer.fit(X_train, Y_train)\n",
    "\n",
    "# #Evaluate model on the test data\n",
    "# visualizer.score(X_test, Y_test)\n",
    "\n",
    "# visualizer.show()                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A __Prediction Error Plot__ shows the actual vales from the dataset against the predicted values generated by the model.\n",
    "- This allows us to see how much variance is in the model.\n",
    "- The Line of Best Fit is used to express a relationship in a scatter plot of different data points.\n",
    "- It is an output of regression analysis and can be used as a prediction tool for indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the standardised residuals resid() & standardised predicted values fittedvalues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals  = model1.resid\n",
    "\n",
    "fittedv   = model1.fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the residuals versus the predicted values using seaborn’s residplot with fitted values as the x parameter, and the dependent variable as y, specify lowess=True. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(fittedv,residuals, lowess=True, color='maroon')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "plt.ylabel('Fitted Values')\n",
    "plt.xlabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### OOP for Data Science\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Outline\n",
    "\n",
    "It is efficient to put machine learning models and other data science techniques into classes so that we can reuse them later and change attributes without changing the code behind these models. Independent concepts can also be put into independent classes: for example, the functioning of a cross-validate class should not affect the functioning of a linear regression class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a class called __ErrorCalculator__ that has methods to compute __the residuals, standardised residuals, Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).__ \n",
    "Name these methods __get_residuals__, __get_standardised_residuals__, __get_mse__ and __get_rmse__ respectively. \n",
    "You can also have a method, __error_summary__ that prints the average, minimum and maximum of the standardised residuals, as well as the MSE and RMSE.\n",
    "The class should have the following parameters:\n",
    "\n",
    "      y: A 1D array of the target variable, size n_observations\n",
    "      y_pred: A 1D array of the predicted values of the target variable, size n_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ErrorCalculator import ErrorCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a generic class called __Plotter__. This class should have a method, run_calculations, to calculate the residuals if they have not yet been calculated, and a method plot, which simply plots a histogram of the residuals.\n",
    "As before, the class should have the following parameters:\n",
    "\n",
    "     y: A 1D array of the target variable, size n_observations\n",
    "     y_pred: A 1D array of the predicted values of the target variable, size n_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create two child classes, __HistogramPlotter__ and __ScatterPlotter__, that both inherit from Plotter. As the name suggests, HistogramPlotter.plot() should return a histogram of the residuals, whereas ScatterPlotter.plot() should return scatterplots of the residual versus predicted values and the predicted versus observed values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
